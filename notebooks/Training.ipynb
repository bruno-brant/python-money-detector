{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================================\n",
      " PyTorch version:      1.12.1+cu116\n",
      " Torchvision version:  0.13.1+cu116\n",
      " Device:               cuda:0\n",
      "=======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Train torchvision with our dataset:\n",
    "# https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
    "# https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "\n",
    "# Torch enable cuda\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"=======================================================================\")\n",
    "print(\" PyTorch version:     \", torch.__version__)\n",
    "print(\" Torchvision version: \", torchvision.__version__)\n",
    "print(\" Device:              \", device)\n",
    "print(\"=======================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Number of classes in the dataset'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLASSES = [5, 10, 25, 50, 100] \t\t\t\t\t\t# We DON'T have background class...\n",
    "\"\"\"All the classes that are in the dataset\"\"\"\n",
    "\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\"\"\"Number of classes in the dataset\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `torch.data.Dataset` class to represent the dataset that we are using. \n",
    "\n",
    "This class implements both `__len__` and `__getitem__` to allow for consumers to obtain dataset items.\n",
    "\n",
    "* `__len__` should return the number of items in the dataset. This is the number of annotated images in the VIA json.\n",
    "* `__getitem__` returns a tuple containing the image data as well as its metadata. For now, we will return the bounding boxes of items in the dataset as well as its labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import money_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 97\n",
      "Test dataset size: 50\n"
     ]
    }
   ],
   "source": [
    "#from utils import collate_fn\n",
    "from torch import utils\n",
    "from torch.utils import data\n",
    "\n",
    "from utils import collate_fn\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((1500, 2000))\n",
    "])\n",
    "\n",
    "# use our dataset and defined transformations\n",
    "dataset_train = CoinsDataset('../assets/dataset/TCC_MBA_Coins.json', '../assets/dataset/moedas/', transform=transform)\n",
    "dataset_test = CoinsDataset('../assets/dataset/TCC_MBA_Coins.json', '../assets/dataset/moedas/', transform=transform)\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset_train)).tolist()\n",
    "dataset_train = data.Subset(dataset_train, indices[:-50])\n",
    "dataset_test = data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# Print the sizes of the datasets\n",
    "print(f'Train dataset size: {len(dataset_train)}')\n",
    "print(f'Test dataset size: {len(dataset_test)}')\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader_train = data.DataLoader(\n",
    "    dataset_train, batch_size=2,#)#, shuffle=True, num_workers=2)\n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "data_loader_test = data.DataLoader(\n",
    "    dataset_test, batch_size=2,#), shuffle=False, num_workers=2)\n",
    "    collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning a model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# load a model pre-trained on COCO\n",
    "weights = torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=weights)\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = len(CLASSES)\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "# from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "# import torchvision\n",
    "\n",
    "# weights = torchvision.models.detection.MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "\n",
    "# def get_instance_segmentation_model(num_classes):\n",
    "#     # load an instance segmentation model pre-trained on COCO\n",
    "#     model = torchvision.models.detection.maskrcnn_resnet50_fpn(\n",
    "#         pretrained=weights)\n",
    "\n",
    "#     # get the number of input features for the classifier\n",
    "#     in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "#     # replace the pre-trained head with a new one\n",
    "#     model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "#     # now get the number of input features for the mask classifier\n",
    "#     in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "#     hidden_layer = 256\n",
    "#     # and replace the mask predictor with a new one\n",
    "#     model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "#                                                        hidden_layer,\n",
    "#                                                        num_classes)\n",
    "\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's instantiate the model and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# get the model using our helper function\n",
    "#model = get_instance_segmentation_model(num_classes)\n",
    "# move model to the device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor):\n",
    "    def f(x):\n",
    "        if x >= warmup_iters:\n",
    "            return 1\n",
    "        alpha = float(x) / warmup_iters\n",
    "        return warmup_factor * (1 - alpha) + alpha\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(device: torch.device, image_list: List[torch.Tensor]):\n",
    "    idx = 0\n",
    "\n",
    "    for image in image_list:\n",
    "        idx += 1\n",
    "        print(f'moving image {idx} to device')\n",
    "        yield image.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from torch import Tensor\n",
    "\n",
    "import sys\n",
    "import math\n",
    "\n",
    "\n",
    "def train_one_epoch(model: torch.nn.Module, optimizer: torch.optim.Optimizer, data_loader: data.DataLoader[DatasetItem], device: torch.device, epoch: int, print_freq: int):\n",
    "    #metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
    "    #metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    #header = 'Epoch: [{}]'.format(epoch)\n",
    "\n",
    "    lr_scheduler = None\n",
    "\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1. / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = warmup_lr_scheduler(\n",
    "            optimizer, warmup_iters, warmup_factor)\n",
    "\n",
    "    # metric_logger.log_every(data_loader, print_freq, header):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    print('Iterating through data loader')\n",
    "    for item in data_loader:\n",
    "        # All this effort to type the goddamn thing\n",
    "        images: List[Tensor] = item[0]\n",
    "\n",
    "        print('images_type', type(images))\n",
    "        print('images', images)\n",
    "\n",
    "        targets: List[Dict[str, Tensor]] = item[1]\n",
    "\n",
    "        print('targets_type:', type(targets))\n",
    "        print('targets:', targets)\n",
    "\n",
    "        images = list(load_images(device, images))\n",
    "        targets = [{k: v.to(device) for k, v in target.items()}\n",
    "                   for target in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # reduce losses over all GPUs for logging purposes\n",
    "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
    "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
    "\n",
    "        loss_value = losses_reduced.item()\n",
    "\n",
    "        print('here3')\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            print(loss_dict_reduced)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        #metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
    "        # metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    # return metric_logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating through data loader\n",
      "Returning items: torch.Size([3, 1500, 2000]) boxes: torch.Size([21, 4]) labels torch.Size([21])\n",
      "Returning items: torch.Size([3, 1500, 2000]) boxes: torch.Size([37, 4]) labels torch.Size([37])\n",
      "images_type <class 'tuple'>\n",
      "images (tensor([[[0.6332, 0.6529, 0.6480,  ..., 0.4528, 0.4792, 0.4824],\n",
      "         [0.6322, 0.6521, 0.6446,  ..., 0.4823, 0.4824, 0.4936],\n",
      "         [0.6398, 0.6693, 0.6535,  ..., 0.4865, 0.5230, 0.4907],\n",
      "         ...,\n",
      "         [0.2736, 0.3204, 0.2635,  ..., 0.5286, 0.5238, 0.5277],\n",
      "         [0.3080, 0.2832, 0.3033,  ..., 0.5298, 0.5064, 0.4835],\n",
      "         [0.3245, 0.2991, 0.3062,  ..., 0.5352, 0.5157, 0.5271]],\n",
      "\n",
      "        [[0.6802, 0.7000, 0.6911,  ..., 0.4763, 0.5027, 0.5059],\n",
      "         [0.6793, 0.6992, 0.6878,  ..., 0.5059, 0.5060, 0.5171],\n",
      "         [0.6868, 0.7164, 0.6967,  ..., 0.5100, 0.5465, 0.5142],\n",
      "         ...,\n",
      "         [0.2423, 0.2811, 0.2243,  ..., 0.5600, 0.5551, 0.5591],\n",
      "         [0.2766, 0.2439, 0.2523,  ..., 0.5612, 0.5377, 0.5149],\n",
      "         [0.2990, 0.2599, 0.2522,  ..., 0.5666, 0.5471, 0.5585]],\n",
      "\n",
      "        [[0.6645, 0.6843, 0.6754,  ..., 0.4606, 0.4870, 0.4902],\n",
      "         [0.6636, 0.6835, 0.6721,  ..., 0.4902, 0.4903, 0.5014],\n",
      "         [0.6712, 0.7007, 0.6810,  ..., 0.4943, 0.5308, 0.4985],\n",
      "         ...,\n",
      "         [0.1913, 0.2341, 0.1851,  ..., 0.6031, 0.5983, 0.6022],\n",
      "         [0.2256, 0.1969, 0.2170,  ..., 0.6043, 0.5809, 0.5580],\n",
      "         [0.2451, 0.2128, 0.2170,  ..., 0.6097, 0.5902, 0.6016]]]), tensor([[[0.7725, 0.7343, 0.7098,  ..., 0.3588, 0.3853, 0.3824],\n",
      "         [0.7667, 0.7324, 0.7039,  ..., 0.3735, 0.3873, 0.3765],\n",
      "         [0.7392, 0.7157, 0.7059,  ..., 0.3706, 0.3784, 0.3500],\n",
      "         ...,\n",
      "         [0.5833, 0.5863, 0.5804,  ..., 0.6500, 0.6186, 0.5892],\n",
      "         [0.5892, 0.5804, 0.5824,  ..., 0.5912, 0.6157, 0.6167],\n",
      "         [0.5892, 0.5843, 0.5745,  ..., 0.5382, 0.5422, 0.5324]],\n",
      "\n",
      "        [[0.8078, 0.7696, 0.7451,  ..., 0.0882, 0.1225, 0.1314],\n",
      "         [0.8020, 0.7676, 0.7392,  ..., 0.1029, 0.1245, 0.1255],\n",
      "         [0.7745, 0.7510, 0.7412,  ..., 0.1000, 0.1157, 0.0990],\n",
      "         ...,\n",
      "         [0.5716, 0.5745, 0.5686,  ..., 0.6255, 0.5971, 0.5696],\n",
      "         [0.5775, 0.5686, 0.5706,  ..., 0.5716, 0.6020, 0.6049],\n",
      "         [0.5775, 0.5725, 0.5627,  ..., 0.5265, 0.5265, 0.5167]],\n",
      "\n",
      "        [[0.8745, 0.8363, 0.8118,  ..., 0.1588, 0.1892, 0.1941],\n",
      "         [0.8686, 0.8343, 0.8059,  ..., 0.1735, 0.1912, 0.1882],\n",
      "         [0.8412, 0.8176, 0.8078,  ..., 0.1706, 0.1824, 0.1618],\n",
      "         ...,\n",
      "         [0.5049, 0.5078, 0.5020,  ..., 0.5931, 0.5735, 0.5539],\n",
      "         [0.5108, 0.5020, 0.5039,  ..., 0.5480, 0.5833, 0.5853],\n",
      "         [0.5108, 0.5059, 0.4961,  ..., 0.5069, 0.5147, 0.5049]]]))\n",
      "targets_type: <class 'tuple'>\n",
      "targets: ({'boxes': tensor([[ 763.6210, 1177.8020, 1272.3790, 1612.1980],\n",
      "        [-146.0000,   90.0000,  168.0000,  408.0000],\n",
      "        [1486.9771, 1058.6310, 1971.0229, 1475.3690],\n",
      "        [1148.0000,  582.0000, 1604.0000,  936.0000],\n",
      "        [1098.0000,  892.0000, 1578.0000, 1320.0000],\n",
      "        [1177.9340, 2069.3350, 1750.0660, 2602.6650],\n",
      "        [1192.0000, 1368.0000, 1718.0000, 1856.0000],\n",
      "        [ 716.9340, 1571.3140, 1289.0660, 2058.6860],\n",
      "        [  83.7430, 2267.7881,  750.2570, 2928.2119],\n",
      "        [ 204.8070, 1752.2080,  805.1930, 2345.7920],\n",
      "        [ 204.8070, 1752.2080,  805.1930, 2345.7920],\n",
      "        [1489.5291, 1697.1949, 2040.4709, 2198.8049],\n",
      "        [ 383.0000, 2723.0000, 1033.0000, 3397.0000],\n",
      "        [1517.0000,  731.0000, 1927.0000, 1077.0000],\n",
      "        [1889.0000,  773.0000, 2455.0000, 1229.0000],\n",
      "        [2090.0000, 1165.0000, 2658.0000, 1625.0000],\n",
      "        [1724.0000, 1376.0000, 2208.0000, 1790.0000],\n",
      "        [ 682.0000, 2144.0000, 1232.0000, 2684.0000],\n",
      "        [1989.0000, 1686.0000, 2631.0000, 2272.0000],\n",
      "        [2386.0000, 2117.0000, 2936.0000, 2623.0000],\n",
      "        [1686.7140, 2207.3000, 2357.2859, 2920.7000]]), 'labels': tensor([  5, 100,   5,   5,  50,  50,  25,  25,  25,  25,  25,   5,  50,  10,\n",
      "        100,  50,  10,   5,  25,  10, 100])}, {'boxes': tensor([[2023.0310,  759.0310, 2342.9690, 1078.9690],\n",
      "        [1489.0000,  988.0000, 1813.0000, 1312.0000],\n",
      "        [ 825.0000,  903.0000, 1141.0000, 1219.0000],\n",
      "        [2148.1709, 2579.1709, 2475.8291, 2906.8291],\n",
      "        [ 784.0000, 2218.0000, 1134.0000, 2568.0000],\n",
      "        [ 456.0000, 2367.0000,  806.0000, 2717.0000],\n",
      "        [1221.6440, 1991.6440, 1588.3560, 2358.3560],\n",
      "        [2115.0000, 1036.0000, 2467.0000, 1388.0000],\n",
      "        [1796.0000, 1071.0000, 2148.0000, 1423.0000],\n",
      "        [1117.0000, 2870.0000, 1467.0000, 3220.0000],\n",
      "        [1243.0000, 1253.0000, 1595.0000, 1605.0000],\n",
      "        [2008.1310, 1919.1310, 2373.8689, 2284.8689],\n",
      "        [1699.0000,  752.0000, 2051.0000, 1104.0000],\n",
      "        [ 850.6440, 1854.6440, 1217.3560, 2221.3560],\n",
      "        [ 839.0000, 1195.0000, 1245.0000, 1601.0000],\n",
      "        [1080.0000, 1559.0000, 1486.0000, 1965.0000],\n",
      "        [2004.2050, 1624.2050, 2331.7949, 1951.7950],\n",
      "        [1794.0000,  348.0000, 2128.0000,  682.0000],\n",
      "        [1100.0000, 2314.0000, 1438.0000, 2652.0000],\n",
      "        [1455.5370, 3331.5371, 1822.4630, 3698.4629],\n",
      "        [1740.0000, 1799.0000, 2078.0000, 2137.0000],\n",
      "        [1719.0000, 2141.0000, 2057.0000, 2479.0000],\n",
      "        [ 259.0000, 1591.0000,  615.0000, 1947.0000],\n",
      "        [1361.0000,  707.0000, 1685.0000, 1031.0000],\n",
      "        [1531.0000, 1456.0000, 1855.0000, 1780.0000],\n",
      "        [1437.0000, 1749.0000, 1761.0000, 2073.0000],\n",
      "        [ 750.5430, 1549.5430, 1095.4570, 1894.4570],\n",
      "        [1480.0000, 2354.0000, 1818.0000, 2692.0000],\n",
      "        [1051.6230,  603.6230, 1402.3770,  954.3770],\n",
      "        [ 903.0000, 2577.0000, 1271.0000, 2945.0000],\n",
      "        [ 641.0000, 2806.0000, 1009.0000, 3174.0000],\n",
      "        [1477.2740, 2685.2739, 1788.7260, 2996.7261],\n",
      "        [1684.6230, 2897.6230, 2035.3770, 3248.3770],\n",
      "        [ 434.6230, 1888.6230,  785.3770, 2239.3770],\n",
      "        [1110.6230,  948.6230, 1461.3770, 1299.3770],\n",
      "        [1806.0000, 1403.0000, 2138.0000, 1735.0000],\n",
      "        [1824.0000, 2443.0000, 2178.0000, 2797.0000]]), 'labels': tensor([ 50,   5,   5,  25,  25,  25,  25,  25,  25,  25,  25,  25,  25,  25,\n",
      "        100, 100,   5,  50,   5,  25,  50,  50,   5,   5,   5,   5,   5,  50,\n",
      "         10,  50,  10,   5,  10,  10,  10,  10,  25])})\n",
      "moving image 1 to device\n",
      "moving image 2 to device\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\OneDrive\\Source\\mba\\15. TCC\\src\\Training.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/OneDrive/Source/mba/15.%20TCC/src/Training.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mengine\u001b[39;00m \u001b[39mimport\u001b[39;00m evaluate\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/OneDrive/Source/mba/15.%20TCC/src/Training.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(NUM_EPOCHS):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/OneDrive/Source/mba/15.%20TCC/src/Training.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# train for one epoch, printing every 10 iterations\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/OneDrive/Source/mba/15.%20TCC/src/Training.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     train_one_epoch(model, optimizer, data_loader_train,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/OneDrive/Source/mba/15.%20TCC/src/Training.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                     device, epoch, print_freq\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/OneDrive/Source/mba/15.%20TCC/src/Training.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# update the learning rate\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/OneDrive/Source/mba/15.%20TCC/src/Training.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     lr_scheduler\u001b[39m.\u001b[39mstep()\n",
      "\u001b[1;32md:\\OneDrive\\Source\\mba\\15. TCC\\src\\Training.ipynb Cell 16\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/Source/mba/15.%20TCC/src/Training.ipynb#X21sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m images \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(load_images(device, images))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/Source/mba/15.%20TCC/src/Training.ipynb#X21sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m targets \u001b[39m=\u001b[39m [{k: v\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m target\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/Source/mba/15.%20TCC/src/Training.ipynb#X21sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m            \u001b[39mfor\u001b[39;00m target \u001b[39min\u001b[39;00m targets]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/OneDrive/Source/mba/15.%20TCC/src/Training.ipynb#X21sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m loss_dict \u001b[39m=\u001b[39m model(images, targets)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/Source/mba/15.%20TCC/src/Training.ipynb#X21sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m losses \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(loss \u001b[39mfor\u001b[39;00m loss \u001b[39min\u001b[39;00m loss_dict\u001b[39m.\u001b[39mvalues())\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/Source/mba/15.%20TCC/src/Training.ipynb#X21sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m# reduce losses over all GPUs for logging purposes\u001b[39;00m\n",
      "File \u001b[1;32md:\\OneDrive\\Source\\mba\\15. TCC\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\OneDrive\\Source\\mba\\15. TCC\\.venv\\lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:105\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m    103\u001b[0m     features \u001b[39m=\u001b[39m OrderedDict([(\u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m, features)])\n\u001b[0;32m    104\u001b[0m proposals, proposal_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrpn(images, features, targets)\n\u001b[1;32m--> 105\u001b[0m detections, detector_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroi_heads(features, proposals, images\u001b[39m.\u001b[39;49mimage_sizes, targets)\n\u001b[0;32m    106\u001b[0m detections \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39mpostprocess(detections, images\u001b[39m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[39m# type: ignore[operator]\u001b[39;00m\n\u001b[0;32m    108\u001b[0m losses \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32md:\\OneDrive\\Source\\mba\\15. TCC\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\OneDrive\\Source\\mba\\15. TCC\\.venv\\lib\\site-packages\\torchvision\\models\\detection\\roi_heads.py:772\u001b[0m, in \u001b[0;36mRoIHeads.forward\u001b[1;34m(self, features, proposals, image_shapes, targets)\u001b[0m\n\u001b[0;32m    770\u001b[0m     \u001b[39mif\u001b[39;00m regression_targets \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    771\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mregression_targets cannot be None\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 772\u001b[0m     loss_classifier, loss_box_reg \u001b[39m=\u001b[39m fastrcnn_loss(class_logits, box_regression, labels, regression_targets)\n\u001b[0;32m    773\u001b[0m     losses \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mloss_classifier\u001b[39m\u001b[39m\"\u001b[39m: loss_classifier, \u001b[39m\"\u001b[39m\u001b[39mloss_box_reg\u001b[39m\u001b[39m\"\u001b[39m: loss_box_reg}\n\u001b[0;32m    774\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\OneDrive\\Source\\mba\\15. TCC\\.venv\\lib\\site-packages\\torchvision\\models\\detection\\roi_heads.py:36\u001b[0m, in \u001b[0;36mfastrcnn_loss\u001b[1;34m(class_logits, box_regression, labels, regression_targets)\u001b[0m\n\u001b[0;32m     31\u001b[0m classification_loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(class_logits, labels)\n\u001b[0;32m     33\u001b[0m \u001b[39m# get indices that correspond to the regression targets for\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m# the corresponding ground truth labels, to be used with\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39m# advanced indexing\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m sampled_pos_inds_subset \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mwhere(labels \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     37\u001b[0m labels_pos \u001b[39m=\u001b[39m labels[sampled_pos_inds_subset]\n\u001b[0;32m     38\u001b[0m N, num_classes \u001b[39m=\u001b[39m class_logits\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "# let's train it for 10 epochs\n",
    "from engine import evaluate\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader_train,\n",
    "                    device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "    print(f'Epoch {epoch} finished.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7f52dee16cfe0c6d4be518e3fb68450c8154cbd9c3f73f0482143bf72f6a6df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
