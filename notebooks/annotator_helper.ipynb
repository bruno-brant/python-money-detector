{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotator Helper\n",
    "This notebook uses a trained model to help us annotate images. \n",
    "\n",
    "It'll find the images on the vgg file that have no annotations, load those images, apply the model to them, and then will save it back to the json file for further exploration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if '..' not in sys.path:\n",
    "\tsys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from typing import Dict, Iterable, List, Tuple, TypedDict, Union, cast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import reload\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.models\n",
    "from PIL import Image, ImageOps\n",
    "from torch import Tensor, tensor\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload money_counter\n",
    "\n",
    "from money_counter import constants, data, models, via_utils, engine\n",
    "from money_counter.data import ViaDatasetOnlyAnnotated, to_via_transform\n",
    "from money_counter.engine import apply_nms\n",
    "from money_counter.models import PredictedTarget, Target\n",
    "from money_counter.utils import decode_data\n",
    "from vgg_image_annotation import v2\n",
    "from vgg_image_annotation.v2 import ImageMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 0.3290896941936272)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model and weights\n",
    "model, model_name = models.get_fasterrcnn_pretrained()\n",
    "version_manager = models.VersionManager('../model_state/')\n",
    "\n",
    "version_manager.load_model(model_name, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "DATASET_PATH = os.environ['COINS_DATASET_PATH']\n",
    "\n",
    "# Load data from annotations\n",
    "via_file = v2.load_via_v2_file(DATASET_PATH)\n",
    "\n",
    "transform = data.ComposeViaTransform([\n",
    "    #transforms.Resize((224, 224)),\n",
    "    to_via_transform(transforms.ToTensor()),\n",
    "])\n",
    "\n",
    "def not_annotated(images_metadata: List[v2.ImageMetadata]) -> List[v2.ImageMetadata]:\n",
    "    return list(filter(lambda image_metadata: not via_utils.is_annotated(image_metadata), images_metadata))\n",
    "\n",
    "\n",
    "dataset = ViaDatasetOnlyAnnotated(DATASET_PATH, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, collate_fn=data.collate_into_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label: i for i, label in enumerate(constants.CLASSES)}\n",
    "label_map_inverted = {v: k for k, v in label_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [21], line 67\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m outputs:\n\u001b[0;32m     64\u001b[0m             metadata_partial\u001b[39m.\u001b[39mappend(output)\n\u001b[1;32m---> 67\u001b[0m main()\n",
      "File \u001b[1;32me:\\source\\money-counter\\python-money-detector\\.venv\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn [21], line 26\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m():\n\u001b[1;32m---> 26\u001b[0m     \u001b[39mfor\u001b[39;00m images, targets \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m     27\u001b[0m         \u001b[39m# move images to the device for inference\u001b[39;00m\n\u001b[0;32m     28\u001b[0m         images \u001b[39m=\u001b[39m [image\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images]\n\u001b[0;32m     29\u001b[0m         \u001b[39m# Move targets to the device for inference\u001b[39;00m\n",
      "File \u001b[1;32me:\\source\\money-counter\\python-money-detector\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32me:\\source\\money-counter\\python-money-detector\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32me:\\source\\money-counter\\python-money-detector\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32me:\\source\\money-counter\\python-money-detector\\.venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32me:\\source\\money-counter\\python-money-detector\\notebooks\\..\\money_counter\\data.py:90\u001b[0m, in \u001b[0;36mViaDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39m\"\"\"Return the image and its metadata at the given index.\"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_images_names[idx]\n\u001b[1;32m---> 90\u001b[0m image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_image(filename)\n\u001b[0;32m     91\u001b[0m metadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_images_metadata[idx]\n\u001b[0;32m     92\u001b[0m target \u001b[39m=\u001b[39m to_target(metadata, image\u001b[39m.\u001b[39msize, _label_map, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilename_map)\n",
      "File \u001b[1;32me:\\source\\money-counter\\python-money-detector\\notebooks\\..\\money_counter\\data.py:103\u001b[0m, in \u001b[0;36mViaDataset._get_image\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    101\u001b[0m image_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_root_dir, filename)\n\u001b[0;32m    102\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(image_path)\n\u001b[1;32m--> 103\u001b[0m image \u001b[39m=\u001b[39m ImageOps\u001b[39m.\u001b[39;49mexif_transpose(image)\n\u001b[0;32m    104\u001b[0m image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[39mreturn\u001b[39;00m image\n",
      "File \u001b[1;32me:\\source\\money-counter\\python-money-detector\\.venv\\lib\\site-packages\\PIL\\ImageOps.py:616\u001b[0m, in \u001b[0;36mexif_transpose\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m    612\u001b[0m                 transposed_image\u001b[39m.\u001b[39minfo[\u001b[39m\"\u001b[39m\u001b[39mXML:com.adobe.xmp\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    613\u001b[0m                     pattern, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, transposed_image\u001b[39m.\u001b[39minfo[\u001b[39m\"\u001b[39m\u001b[39mXML:com.adobe.xmp\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    614\u001b[0m                 )\n\u001b[0;32m    615\u001b[0m     \u001b[39mreturn\u001b[39;00m transposed_image\n\u001b[1;32m--> 616\u001b[0m \u001b[39mreturn\u001b[39;00m image\u001b[39m.\u001b[39;49mcopy()\n",
      "File \u001b[1;32me:\\source\\money-counter\\python-money-detector\\.venv\\lib\\site-packages\\PIL\\Image.py:1185\u001b[0m, in \u001b[0;36mImage.copy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcopy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1178\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m \u001b[39m    Copies this image. Use this method if you wish to paste things\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m \u001b[39m    into an image, but still retain the original.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1183\u001b[0m \u001b[39m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1185\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   1186\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mim\u001b[39m.\u001b[39mcopy())\n",
      "File \u001b[1;32me:\\source\\money-counter\\python-money-detector\\.venv\\lib\\site-packages\\PIL\\ImageFile.py:260\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m    255\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mimage file is truncated \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    256\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(b)\u001b[39m}\u001b[39;00m\u001b[39m bytes not processed)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m         )\n\u001b[0;32m    259\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m+\u001b[39m s\n\u001b[1;32m--> 260\u001b[0m n, err_code \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(b)\n\u001b[0;32m    261\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    262\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run the model for the images in the dataset\n",
    "device = engine.get_device()\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def to_shape(tensor: Tensor):\n",
    "    tensor = tensor.numpy()\n",
    "    return {\n",
    "        'name': 'rect',\n",
    "        'x': float(tensor[0]),\n",
    "        'y': float(tensor[1]),\n",
    "        'width': float(tensor[2] - tensor[0]),\n",
    "        'height': float(tensor[3] - tensor[1])\n",
    "    }\n",
    "\n",
    "\n",
    "metadata_partial: List[ImageMetadata] = []\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def main():\n",
    "    for images, targets in dataloader:\n",
    "        # move images to the device for inference\n",
    "        images = [image.to(device) for image in images]\n",
    "        # Move targets to the device for inference\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        # Apply model\n",
    "        outputs = cast(List[PredictedTarget], model(images))\n",
    "\n",
    "        # Apply NMS\n",
    "        apply_nms(outputs)\n",
    "\n",
    "        # Convert back the outputs to cpu\n",
    "        outputs = [{k: v.to('cpu') for k, v in cast(\n",
    "            List[Tuple[str, Tensor]], t.items())} for t in outputs]\n",
    "\n",
    "        # Decode the filename\n",
    "        images_id = [t['image_id'] for t in targets]\n",
    "        images_filename = [\n",
    "            *decode_data(dataset.filename_map, [i.item() for i in images_id])]\n",
    "\n",
    "        # Associate to the targets\n",
    "        outputs = [output | {'filename': filename}\n",
    "                   for filename, output in zip(images_filename, outputs)]\n",
    "\n",
    "        # Now convert back the bounding boxes to the original format\n",
    "        outputs = [{\n",
    "            'filename': output['filename'],\n",
    "            'regions': [{\n",
    "                'shape_attributes': to_shape(box),\n",
    "                'region_attributes': {\n",
    "                    \"Value\": label_map_inverted[int(label.item())],\n",
    "                    \"Edition\": \"Unknown\",\n",
    "                    \"Side\": \"Unknown\"\n",
    "                }\n",
    "            } for box, score, label in zip(output['boxes'], output['scores'], output['labels']) if score > 0.35]\n",
    "        } for output in outputs]\n",
    "\n",
    "        for output in outputs:\n",
    "            metadata_partial.append(output)\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(filter(lambda x: not x['regions'], metadata_partial))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index the metadata\n",
    "d = {}\n",
    "for item in metadata_partial:\n",
    "\td[item['filename']] = item\n",
    "\n",
    "metadata_partial = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json with the metadata\n",
    "via = v2.load_via_v2_file(DATASET_PATH)\n",
    "\n",
    "for metadata in via['_via_img_metadata'].values():\n",
    "\tif metadata['filename'] in metadata_partial:\n",
    "\t\tmetadata['regions'] = metadata_partial[metadata['filename']]['regions']\n",
    "\n",
    "# Save the new json\n",
    "with open(f'{DATASET_PATH}/coins_2.json', 'w') as f:\n",
    "\tjson.dump(via, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dca31d16848f20992eb89f8a7472940a952ffc02113be43df95621585f721965"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
