{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotator Helper\n",
    "This notebook will use a model to help us annotate images. \n",
    "\n",
    "It'll find the images on the vgg file that have no annotations, load those images, apply the model to them, and then will save it back to the json file for further exploration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "from money_counter import data, models, via_utils\n",
    "from vgg_image_annotation import v2\n",
    "from PIL import Image, ImageOps\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "from typing import Iterable, List\n",
    "from money_counter.data import CoinsDataset\n",
    "from money_counter.utils import decode_data\n",
    "from money_counter import constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ../model_state//fasterrcnn_resnet50_fpn-pretrained/epoch_001.pth\n",
      "Loaded model from ../model_state//fasterrcnn_resnet50_fpn-pretrained/epoch_001.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(40, 0.22741659009648907)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model and weights\n",
    "model, model_name = models.get_fasterrcnn_pretrained()\n",
    "version_manager = models.VersionManager('../model_state/')\n",
    "\n",
    "version_manager.load_model(model_name, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "DATASET_DIR = \"../assets/coins_dataset\"\n",
    "\n",
    "# Load data from annotations\n",
    "via_file = v2.load_via_v2_file(f'{DATASET_DIR}/coins.json')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def not_annotated(images_metadata: List[v2.ImageMetadata]) -> List[v2.ImageMetadata]:\n",
    "    return list(filter(lambda image_metadata: not via_utils.is_annotated(image_metadata), images_metadata))\n",
    "\n",
    "\n",
    "dataset = CoinsDataset('../assets/coins_dataset/coins.json', '../assets/coins_dataset/images/',\n",
    "                       transform=transform, metadata_transform=not_annotated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, collate_fn=data.collate_into_lists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label: i for i, label in enumerate(constants.CLASSES)}\n",
    "label_map_inverted = {v: k for k, v in label_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model for the images in the dataset\n",
    "\n",
    "import json\n",
    "from typing import Tuple, TypedDict, Union, cast\n",
    "\n",
    "from torch import Tensor, tensor\n",
    "from money_counter.models import PredictedTarget, Target\n",
    "from money_counter.engine import apply_nms\n",
    "from vgg_image_annotation.v2 import ImageMetadata\n",
    "\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def to_shape(tensor: Tensor):\n",
    "    tensor = tensor.numpy()\n",
    "    return {\n",
    "        'name': 'rect',\n",
    "        'x': float(tensor[0]),\n",
    "        'y': float(tensor[1]),\n",
    "        'width': float(tensor[2] - tensor[0]),\n",
    "        'height': float(tensor[3] - tensor[1])\n",
    "    }\n",
    "\n",
    "\n",
    "metadata_partial: List[ImageMetadata] = []\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def main():\n",
    "    for images, targets in dataloader:\n",
    "        # move images to the device for inference\n",
    "        images = [image.to(device) for image in images]\n",
    "        # Move targets to the device for inference\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        # Apply model\n",
    "        outputs = cast(List[PredictedTarget], model(images))\n",
    "\n",
    "        # Apply NMS\n",
    "        apply_nms(outputs)\n",
    "\n",
    "        # Convert back the outputs to cpu\n",
    "        outputs = [{k: v.to('cpu') for k, v in cast(\n",
    "            List[Tuple[str, Tensor]], t.items())} for t in outputs]\n",
    "\n",
    "        # Decode the filename\n",
    "        images_id = [t['image_id'] for t in targets]\n",
    "        images_filename = [\n",
    "            *decode_data(dataset.filename_map, [i.item() for i in images_id])]\n",
    "\n",
    "        # Associate to the targets\n",
    "        outputs = [output | {'filename': filename}\n",
    "                   for filename, output in zip(images_filename, outputs)]\n",
    "\n",
    "        # Now convert back the bounding boxes to the original format\n",
    "        outputs = [{\n",
    "            'filename': output['filename'],\n",
    "            'regions': [{\n",
    "                'shape_attributes': to_shape(box),\n",
    "                'region_attributes': {\n",
    "                    \"Value\": label_map_inverted[int(label.item())],\n",
    "                    \"Edition\": \"Unknown\",\n",
    "                    \"Side\": \"Unknown\"\n",
    "                }\n",
    "            } for box, score, label in zip(output['boxes'], output['scores'], output['labels']) if score > 0.35]\n",
    "        } for output in outputs]\n",
    "\n",
    "        for output in outputs:\n",
    "            metadata_partial.append(output)\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': '20220922_091534.jpg', 'regions': []}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: not x['regions'], metadata_partial))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index the metadata\n",
    "d = {}\n",
    "for item in metadata_partial:\n",
    "\td[item['filename']] = item\n",
    "\n",
    "metadata_partial = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json with the metadata\n",
    "via = v2.load_via_v2_file(f'{DATASET_DIR}/coins.json')\n",
    "\n",
    "for metadata in via['_via_img_metadata'].values():\n",
    "\tif metadata['filename'] in metadata_partial:\n",
    "\t\tmetadata['regions'] = metadata_partial[metadata['filename']]['regions']\n",
    "\n",
    "# Save the new json\n",
    "with open(f'{DATASET_DIR}/coins_2.json', 'w') as f:\n",
    "\tjson.dump(via, f)\n",
    "\n",
    "\t\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7f52dee16cfe0c6d4be518e3fb68450c8154cbd9c3f73f0482143bf72f6a6df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
